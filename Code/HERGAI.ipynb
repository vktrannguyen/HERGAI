{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1b8c24",
   "metadata": {},
   "source": [
    "# HERGAI: An Artificial Intelligence Tool for Structure-Based Prediction of hERG Inhibitors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb87260",
   "metadata": {},
   "source": [
    "You are using a Jupyter notebook containing the Python code, named **HERGAI**, to predict inhibitors of the human Ether-Ã -go-go-Related Gene (hERG) potassium channel. This code is introduced in our article:\n",
    "\n",
    "**Tran-Nguyen, V.K., Randriharimanamizara, U.F., Taboureau, O. HERGAI: An Artificial Intelligence Tool for Structure-Based Prediction of hERG Inhibitors. (2025)**\n",
    "\n",
    "You can use this code to reproduce our results: all necessary input files are provided in our **HERGAI** GitHub repository (https://github.com/vktrannguyen/HERGAI). You can also make predictions on your own data, even on a large scale. Please read our article (cited above) for more information.\n",
    "\n",
    "To use this Jupyter notebook, you need to set up a proper environment. A suggested solution is to use the *protocol-env.yml* file provided in our **MLSF-protocol** GitHub repository (https://github.com/vktrannguyen/MLSF-protocol) and install a few additional packages (e.g., imblearn). Please refer to the **import** section at the beginning of each code block to ensure that all required Python dependencies are installed beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46c76e",
   "metadata": {},
   "source": [
    "## Part 1: Training and applying base classification models (RF_BC, XGB_BC, DNN_BC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d2a89",
   "metadata": {},
   "source": [
    "To train and apply the following base classification models, we use, as features, the **PLEC fingerprints extracted from docking poses selected by ClassyPose**.\n",
    "\n",
    "For more information on ClassyPose, please refer to this article: https://advanced.onlinelibrary.wiley.com/doi/full/10.1002/aisy.202400238 and our **Classy_Pose** GitHub repository (https://github.com/vktrannguyen/Classy_Pose)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356a84b",
   "metadata": {},
   "source": [
    "### RF_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b12cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Provide the paths to the csv training and test data files:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "test_data = pd.read_csv(\"Provide the file path to test_data.csv\")\n",
    "Train_Class = train_data['activity'].map({'Active': 1, 'Inactive': 0})  \n",
    "Test_Class = test_data['activity'].map({'Active': 1, 'Inactive': 0}) \n",
    "\n",
    "# Provide the paths to the csv training and test features: \n",
    "d_train_csv = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None)\n",
    "d_test_csv = pd.read_csv(\"Provide the file path to test_PLEC_ClassyPose.csv\", header=None)\n",
    "train_features = np.array(d_train_csv)\n",
    "test_features = np.array(d_test_csv)\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    # Count the number of actives and inactives:\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "\n",
    "    # Calculate the number of actives to oversample:\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "\n",
    "    # Create a RandomOverSampler instance:\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "\n",
    "    # Resample the dataset:\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply controlled oversampling to the training data:\n",
    "train_features_resampled, Train_Class_resampled = controlled_oversampling(train_features, Train_Class)\n",
    "\n",
    "# Train RF_BC on the resampled training set using optimal hyperparameters:\n",
    "rf_plec = RandomForestClassifier(\n",
    "    n_estimators=2600, \n",
    "    max_depth=6, \n",
    "    criterion='gini',\n",
    "    max_features='sqrt', \n",
    "    n_jobs=20,\n",
    "    random_state=42\n",
    ")\n",
    "rf_plec.fit(train_features_resampled, Train_Class_resampled)\n",
    "\n",
    "# Test RF_BC on the test molecules:\n",
    "threshold = 0.32799228309084005\n",
    "prediction_test_rf_plec_prob = rf_plec.predict_proba(test_features)\n",
    "prediction_test_rf_plec_class = (prediction_test_rf_plec_prob[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Get classification results on the test molecules and export to csv:\n",
    "plec_result_rf = pd.DataFrame({\n",
    "    \"Active_Prob\": prediction_test_rf_plec_prob[:, 1], \n",
    "    \"Inactive_Prob\": prediction_test_rf_plec_prob[:, 0], \n",
    "    \"Predicted_Class\": [\"Active\" if pred == 1 else \"Inactive\" for pred in prediction_test_rf_plec_class],\n",
    "    \"Real_Class\": Test_Class.map({1: 'Active', 0: 'Inactive'})  \n",
    "})\n",
    "plec_result_rf.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file RF_BC.csv is provided in our GitHub repository, in Data/Test_set/Results/Our_AI_classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d773b78",
   "metadata": {},
   "source": [
    "### XGB_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Provide the paths to the csv training and test data files:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "test_data = pd.read_csv(\"Provide the file path to test_data.csv\")\n",
    "train_data['activity'] = train_data['activity'].map({'Active': 1, 'Inactive': 0})\n",
    "test_data['activity'] = test_data['activity'].map({'Active': 1, 'Inactive': 0})\n",
    "Train_Class = train_data['activity'].astype('int32')\n",
    "Test_Class = test_data['activity'].astype('int32')\n",
    "\n",
    "# Provide the paths to the csv training and test features: \n",
    "train_features = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None).astype('float32')\n",
    "test_features = pd.read_csv(\"Provide the file path to test_PLEC_ClassyPose.csv\", header=None).astype('float32')\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    # Count the number of actives and inactives:\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "\n",
    "    # Calculate the number of actives to oversample:\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "\n",
    "    # Create a RandomOverSampler instance:\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "\n",
    "    # Resample the dataset:\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply controlled oversampling to the training data:\n",
    "train_features_resampled, Train_Class_resampled = controlled_oversampling(train_features, Train_Class)\n",
    "\n",
    "# Train XGB_BC on the resampled training set using optimal hyperparameters:\n",
    "xgb_plec = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    max_depth=4,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=1,\n",
    "    n_estimators=86,\n",
    "    n_jobs=20,  \n",
    "    tree_method='hist',  \n",
    "    grow_policy='depthwise',  \n",
    "    random_state=42\n",
    ")\n",
    "xgb_plec.fit(train_features_resampled, Train_Class_resampled)\n",
    "\n",
    "# Test XGB_BC on the test molecules:\n",
    "threshold = 0.06986083857271219\n",
    "prediction_test_xgb_plec_prob = xgb_plec.predict_proba(test_features)\n",
    "prediction_test_xgb_plec_class = (prediction_test_xgb_plec_prob[:, 1] > threshold).astype(int)\n",
    "\n",
    "# Get classification results on the test molecules and export to csv:\n",
    "plec_result_xgb = pd.DataFrame({\n",
    "    \"Active_Prob\": prediction_test_xgb_plec_prob[:, 1],\n",
    "    \"Inactive_Prob\": prediction_test_xgb_plec_prob[:, 0],\n",
    "    \"Predicted_Class\": prediction_test_xgb_plec_class,\n",
    "    \"Real_Class\": Test_Class\n",
    "})\n",
    "plec_result_xgb.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file XGB_BC.csv is provided in our GitHub repository, in Data/Test_set/Results/Our_AI_classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f86f94",
   "metadata": {},
   "source": [
    "### DNN_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up multiple CPUs:\n",
    "num_cpus = 25\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_cpus)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_cpus)\n",
    "\n",
    "# Provide the paths to the csv training and test data files:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "test_data = pd.read_csv(\"Provide the file path to test_data.csv\")\n",
    "Train_Class = train_data['activity'].map({'Active': 1, 'Inactive': 0})  \n",
    "Test_Class = test_data['activity'].map({'Active': 1, 'Inactive': 0})  \n",
    "\n",
    "# Provide the paths to the csv training and test features:\n",
    "d_train_csv = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None)\n",
    "d_test_csv = pd.read_csv(\"Provide the file path to test_PLEC_ClassyPose.csv\", header=None)\n",
    "train_features = d_train_csv.values\n",
    "test_features = d_test_csv.values\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    # Count the number of actives and inactives\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "\n",
    "    # Calculate the number of actives to oversample\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "\n",
    "    # Create a RandomOverSampler instance\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "\n",
    "    # Resample the dataset\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply controlled oversampling to the training data:\n",
    "train_features_resampled, Train_Class_resampled = controlled_oversampling(train_features, Train_Class)\n",
    "\n",
    "# Train DNN_BC on the resampled training set using optimal hyperparameters:\n",
    "dnn_plec = keras.Sequential()\n",
    "dnn_plec.add(layers.Dense(abs(int(352.8790957431371)), activation='relu'))\n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.3912375678578729))\n",
    "dnn_plec.add(layers.Dense(abs(int(507.20805366280905)), activation='relu'))\n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.4672821567914654))  \n",
    "dnn_plec.add(layers.Dense(abs(int(242.0267251614644)), activation='relu'))  \n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.382443636406835))  \n",
    "dnn_plec.add(layers.Dense(1, activation='sigmoid')) \n",
    "dnn_plec.compile(optimizer='Adadelta', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "dnn_plec.fit(np.array(train_features_resampled), Train_Class_resampled, \n",
    "             epochs=10, batch_size=68, verbose=1)\n",
    "\n",
    "# Test DNN_BC on the test molecules:\n",
    "threshold = 0.14415252008378787\n",
    "test_predictions = dnn_plec.predict(np.array(test_features))\n",
    "prediction_test_dnn_plec_class = [\"Active\" if num > threshold else \"Inactive\" for num in test_predictions]  \n",
    "\n",
    "# Get classification results on the test molecules and export to csv:\n",
    "plec_result_dnn = pd.DataFrame({\"Active_Prob\": test_predictions[:, 0],\n",
    "                                \"Predicted_Class\": prediction_test_dnn_plec_class,\n",
    "                                \"Real_Class\": Test_Class})\n",
    "plec_result_dnn.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file DNN_BC.csv is provided in our GitHub repository, in Data/Test_set/Results/Our_AI_classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d92fbe",
   "metadata": {},
   "source": [
    "## Part 2: Training and applying our best stacking classification model (DNN_SC) from the three base models previously trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa510714",
   "metadata": {},
   "source": [
    "To train and apply our best stacking classification model (DNN_SC), we use, as features, the **adjusted scores from the three base classifiers (RF_BC, XGB_BC, DNN_BC)** for all training and test ligands.\n",
    "\n",
    "For this, the following procedure is followed:\n",
    "\n",
    "- The active probabilities of the ligands in each of the five validation folds (partitioned from the training set) are predicted by each base model, trained on the corresponding training fold, using its respective optimal hyperparameters. \n",
    "\n",
    "- To account for the optimal decision threshold determined for each base model (which changes from one model to another, and impacts their classification results), we adjust the active probabilities predicted by a base model, dividing them by the corresponding decision threshold. \n",
    "\n",
    "This practice gives the adjusted scores that we use as features for DNN_SC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8a101",
   "metadata": {},
   "source": [
    "### Part 2.1: Using each base model to issue an active probability and an adjusted score for each ligand in the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d29eda",
   "metadata": {},
   "source": [
    "#### Using RF_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f13e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Provide the path to the csv training data file:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "train_data['activity'] = train_data['activity'].map({'Active': 1, 'Inactive': 0})\n",
    "Train_Class = train_data['activity'].astype('int32')\n",
    "\n",
    "# Provide the paths to the csv training features:\n",
    "train_features = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None).astype('float32')\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Define RF_BC with its optimal hyperparameters:\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=2600, \n",
    "    max_depth=6, \n",
    "    criterion='gini',\n",
    "    max_features='sqrt', \n",
    "    n_jobs=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold CV:\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "predictions = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_features, Train_Class), start=1):\n",
    "    x_train_fold, x_val_fold = train_features.iloc[train_index], train_features.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = Train_Class.iloc[train_index], Train_Class.iloc[val_index]\n",
    "    val_ids = train_data['SID'].iloc[val_index]  \n",
    "\n",
    "    # Apply controlled oversampling within each fold:\n",
    "    x_train_fold_resampled, y_train_fold_resampled = controlled_oversampling(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Train the model on the resampled data:\n",
    "    model.fit(x_train_fold_resampled, y_train_fold_resampled)\n",
    "\n",
    "    # Get predicted probabilities for the validation fold:\n",
    "    y_scores = model.predict_proba(x_val_fold)[:, 1]  \n",
    "\n",
    "    # Collect the results for each molecule in the validation fold:\n",
    "    fold_results = pd.DataFrame({\n",
    "        'SID': val_ids.values,            \n",
    "        'Actual_Class': y_val_fold.values, \n",
    "        'Predicted_Probability': y_scores,  \n",
    "        'Adjusted_Score': y_scores/0.32799228309084005\n",
    "    })\n",
    "    predictions.append(fold_results)\n",
    "\n",
    "# Concatenate all fold results into a single DataFrame:\n",
    "all_predictions = pd.concat(predictions, ignore_index=True)\n",
    "\n",
    "# Save the predictions to a csv file:\n",
    "all_predictions.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file rf_cv_predictions.csv is provided in our GitHub repository, in Data/Training_set/Stacking_ensemble_ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835eacb",
   "metadata": {},
   "source": [
    "#### Using XGB_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Provide the path to the csv training data file:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "train_data['activity'] = train_data['activity'].map({'Active': 1, 'Inactive': 0})\n",
    "Train_Class = train_data['activity'].astype('int32')\n",
    "\n",
    "# Provide the paths to the csv training features:\n",
    "train_features = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None).astype('float32')\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Define XGB_BC with its optimal hyperparameters:\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    max_depth=4,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=1,\n",
    "    n_estimators=86,\n",
    "    n_jobs=20,  \n",
    "    tree_method='hist',\n",
    "    grow_policy='depthwise',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Perform 5-fold CV:\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "predictions = []  \n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_features, Train_Class), start=1):\n",
    "    x_train_fold, x_val_fold = train_features.iloc[train_index], train_features.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = Train_Class.iloc[train_index], Train_Class.iloc[val_index]\n",
    "    val_ids = train_data['SID'].iloc[val_index]  \n",
    "\n",
    "    # Apply controlled oversampling within each fold:\n",
    "    x_train_fold_resampled, y_train_fold_resampled = controlled_oversampling(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Train the model on the resampled data:\n",
    "    model.fit(x_train_fold_resampled, y_train_fold_resampled)\n",
    "\n",
    "    # Get predicted probabilities for the validation fold:\n",
    "    y_scores = model.predict_proba(x_val_fold)[:, 1]  \n",
    "\n",
    "    # Collect the results for each molecule in the validation fold:\n",
    "    fold_results = pd.DataFrame({\n",
    "        'SID': val_ids.values,            \n",
    "        'Actual_Class': y_val_fold.values, \n",
    "        'Predicted_Probability': y_scores,  \n",
    "        'Adjusted_Score': y_scores/0.06986083857271219  \n",
    "    })\n",
    "    predictions.append(fold_results)\n",
    "\n",
    "# Concatenate all fold results into a single DataFrame:\n",
    "all_predictions = pd.concat(predictions, ignore_index=True)\n",
    "\n",
    "# Save the predictions to a csv file:\n",
    "all_predictions.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file xgb_cv_predictions.csv is provided in our GitHub repository, in Data/Training_set/Stacking_ensemble_ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185e1cd",
   "metadata": {},
   "source": [
    "#### Using DNN_BC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1665e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "import gc\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up multiple CPUs:\n",
    "num_cpus = 25\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_cpus)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_cpus)\n",
    "\n",
    "# Provide the path to the csv training data file:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data.csv\")\n",
    "Train_Class = train_data['activity'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "# Provide the paths to the csv training features:\n",
    "d_train_csv = pd.read_csv(\"Provide the file path to training_PLEC_ClassyPose.csv\", header=None)\n",
    "train_features = d_train_csv.values\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Define DNN_BC with its optimal hyperparameters:\n",
    "def build_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(abs(int(352.8790957431371)), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.3912375678578729))\n",
    "    model.add(layers.Dense(abs(int(507.20805366280905)), activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4672821567914654))  \n",
    "    model.add(layers.Dense(abs(int(242.0267251614644)), activation=\"relu\"))  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.382443636406835))  \n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(optimizer='Adadelta', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Perform 5-fold CV:\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "predictions = [] \n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(train_features, Train_Class), start=1):\n",
    "    print(f\"Processing fold {fold}...\")\n",
    "    \n",
    "    # Reset model to avoid carrying overweights:\n",
    "    model = build_model()\n",
    "    \n",
    "    x_train_fold, x_val_fold = train_features[train_index], train_features[val_index]\n",
    "    y_train_fold, y_val_fold = Train_Class.values[train_index], Train_Class.values[val_index]\n",
    "    val_ids = train_data['SID'].iloc[val_index] \n",
    "\n",
    "    # Apply controlled oversampling within each fold:\n",
    "    x_train_fold_resampled, y_train_fold_resampled = controlled_oversampling(x_train_fold, y_train_fold)\n",
    "\n",
    "    # Train the model on the resampled data:\n",
    "    model.fit(x_train_fold_resampled, y_train_fold_resampled, epochs=10, batch_size=68, verbose=0)\n",
    "\n",
    "    # Get predicted probabilities for the validation fold:\n",
    "    y_scores = model.predict(x_val_fold).flatten()  \n",
    "\n",
    "    # Collect the results for each molecule in the validation fold\n",
    "    fold_results = pd.DataFrame({\n",
    "        'SID': val_ids.values,            \n",
    "        'Actual_Class': y_val_fold, \n",
    "        'Predicted_Probability': y_scores,  \n",
    "        'Adjusted_Score': y_scores/0.14415252008378787  \n",
    "    })\n",
    "    predictions.append(fold_results)\n",
    "    \n",
    "    # Clear session and collect garbage to free memory between folds:\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Concatenate all fold results into a single DataFrame:\n",
    "all_predictions = pd.concat(predictions, ignore_index=True)\n",
    "\n",
    "# Save the predictions to a csv file:\n",
    "all_predictions.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file dnn_cv_predictions.csv is provided in our GitHub repository, in Data/Training_set/Stacking_ensemble_ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1aff5",
   "metadata": {},
   "source": [
    "### Part 2.2: Using the adjusted scores from the three base classifiers to train and apply DNN_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set up multiple CPUs:\n",
    "num_cpus = 25\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_cpus)\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_cpus)\n",
    "\n",
    "# Provide the path to the csv training data file:\n",
    "train_data = pd.read_csv(\"Provide the file path to training_data_stackedML.csv\")\n",
    "# To ensure the order of all training molecules, use the training_data_stackedML.csv file.\n",
    "# This file can be downloaded from our GitHub repository, in Data/Training_set/Stacking_ensemble_ML.\n",
    "\n",
    "# Provide the path to the csv test data file:\n",
    "test_data = pd.read_csv(\"Provide the file path to test_3_base_models.csv\")\n",
    "# The test_3_base_models.csv file contains the output of three base models for each test molecule.\n",
    "# The output includes the active probabilities and the adjusted scores.\n",
    "# You can prepare this file yourself from the three files RF_BC.csv, XGB_BC.csv, DNN_BC.csv obtained above.\n",
    "# You can also download this file from our GitHub repository, in Data/Test_set.\n",
    "\n",
    "# Call the \"activity\" labels of all training and test molecules:\n",
    "Train_Class = train_data['Actual_Class']\n",
    "Test_Class = test_data['Real_Class'].map({'Active': 1, 'Inactive': 0})\n",
    "\n",
    "# Provide the paths to the base model prediction files on the training set:\n",
    "base_model_1 = pd.read_csv(\"Provide the file path to rf_cv_predictions.csv\")\n",
    "base_model_2 = pd.read_csv(\"Provide the file path to xgb_cv_predictions.csv\")\n",
    "base_model_3 = pd.read_csv(\"Provide the file path to dnn_cv_predictions.csv\")\n",
    "\n",
    "# Merge predictions into a single DataFrame\n",
    "ensemble_data = pd.DataFrame({\n",
    "    'SID': base_model_1['SID'],\n",
    "    'Actual_Class': base_model_1['Actual_Class'],\n",
    "    'Base1_AdjScore': base_model_1['Adjusted_Score'],\n",
    "    'Base2_AdjScore': base_model_2['Adjusted_Score'],\n",
    "    'Base3_AdjScore': base_model_3['Adjusted_Score']\n",
    "})\n",
    "\n",
    "# Define training and test features for training and applying DNN_SC:\n",
    "d_train_csv = ensemble_data[['Base1_AdjScore', 'Base2_AdjScore', 'Base3_AdjScore']].values\n",
    "train_features = np.array(d_train_csv)\n",
    "d_test_csv = test_data[['rf_Adjusted_Score', 'xgb_Adjusted_Score', 'dnn_Adjusted_Score']].values\n",
    "test_features = np.array(d_test_csv)\n",
    "\n",
    "# Define a function to perform controlled oversampling of actives:\n",
    "def controlled_oversampling(X, y, ratio=100):\n",
    "    n_actives = np.sum(y == 1)\n",
    "    n_inactives = np.sum(y == 0)\n",
    "    target_n_actives = min(n_actives * ratio, n_inactives)\n",
    "    ros = RandomOverSampler(sampling_strategy={1: target_n_actives}, random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply controlled oversampling to the training data:\n",
    "train_features_resampled, Train_Class_resampled = controlled_oversampling(train_features, Train_Class)\n",
    "\n",
    "# Train DNN_SC on the resampled training set using optimal hyperparameters:\n",
    "dnn_plec = keras.Sequential()\n",
    "dnn_plec.add(layers.Dense(abs(int(131.561382068786)), activation='relu'))\n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.3294908082433152))\n",
    "dnn_plec.add(layers.Dense(abs(int(227.11758746306032)), activation='relu'))\n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.4453136608614301))  \n",
    "dnn_plec.add(layers.Dense(abs(int(81.34015005515602)), activation='relu'))  \n",
    "dnn_plec.add(layers.BatchNormalization())\n",
    "dnn_plec.add(layers.Dropout(0.5668191107970695))  \n",
    "dnn_plec.add(layers.Dense(1, activation='sigmoid')) \n",
    "dnn_plec.compile(optimizer='rmsprop', loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "dnn_plec.fit(np.array(train_features_resampled), Train_Class_resampled, \n",
    "             epochs=10, batch_size=46, verbose=1)\n",
    "\n",
    "# Test DNN_SC on the test molecules:\n",
    "threshold = 0.20844117646625016\n",
    "test_predictions = dnn_plec.predict(np.array(test_features))\n",
    "prediction_test_dnn_plec_class = [\"Active\" if num > threshold else \"Inactive\" for num in test_predictions]\n",
    "\n",
    "# Get classification results on the test molecules and export to csv:\n",
    "plec_result_dnn = pd.DataFrame({\"SID\": test_data['SID'],\n",
    "                                \"Active_Prob\": test_predictions[:, 0],\n",
    "                                \"Predicted_Class\": prediction_test_dnn_plec_class,\n",
    "                                \"Real_Class\": Test_Class})\n",
    "plec_result_dnn.to_csv(\"Provide the file path to your csv output file\", index=False)\n",
    "# The output file DNN_SC.csv is provided in our GitHub repository, in Data/Test_set/Results/Our_AI_classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
